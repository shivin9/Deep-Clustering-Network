{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import umap\n",
    "from DCN import DCN\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "color = ['grey', 'red', 'blue', 'pink', 'brown', 'black', 'magenta', 'purple', 'orange', 'cyan', 'olive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader):\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for data, target in test_loader:\n",
    "        batch_size = data.size()[0]\n",
    "        data = data.view(batch_size, -1).to(model.device)\n",
    "        latent_X = model.autoencoder(data, latent=True)\n",
    "        latent_X = latent_X.detach().cpu().numpy()\n",
    "\n",
    "        y_test.append(target.view(-1, 1).numpy())\n",
    "        y_pred.append(model.clustering.update_assign(latent_X).reshape(-1, 1))\n",
    "    \n",
    "    y_test = np.vstack(y_test).reshape(-1)\n",
    "    y_pred = np.vstack(y_pred).reshape(-1)\n",
    "    return (normalized_mutual_info_score(y_test, y_pred),\n",
    "            adjusted_rand_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "def solver(args, model, train_loader, test_loader):\n",
    "    rec_loss_list = model.pretrain(train_loader, epoch=args.pre_epoch)\n",
    "    nmi_list = []\n",
    "    ari_list = []\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        model.train()\n",
    "        model.fit(e, train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        NMI, ARI = evaluate(model, test_loader)  # evaluation on the test_loader\n",
    "        nmi_list.append(NMI)\n",
    "        ari_list.append(ARI)\n",
    "        \n",
    "        print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(\n",
    "            e+1, NMI, ARI))\n",
    "\n",
    "    return rec_loss_list, nmi_list, ari_list\n",
    "\n",
    "\n",
    "def create_imbalanced_data_clusters(n_samples=1000, n_features=8, n_informative=5, n_classes=2,\\\n",
    "                            n_clusters = 2, frac=0.2, outer_class_sep=0.5, inner_class_sep=0.1, clus_per_class=2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    X = np.empty(shape=n_features)\n",
    "    Y = np.empty(shape=1)\n",
    "    offsets = np.random.normal(0, outer_class_sep, size=(n_clusters, n_features))\n",
    "    for i in range(n_clusters):\n",
    "        samples = int(np.random.normal(n_samples, n_samples/10))\n",
    "        x, y = make_classification(n_samples=samples, n_features=n_features, n_informative=n_informative,\\\n",
    "                                    n_classes=n_classes, class_sep=inner_class_sep, n_clusters_per_class=clus_per_class)\n",
    "                                    # n_repeated=0, n_redundant=0)\n",
    "        x += offsets[i]\n",
    "        y_0 = np.where(y == 0)[0]\n",
    "        y_1 = np.where(y != 0)[0]\n",
    "        y_1 = np.random.choice(y_1, int(np.random.normal(frac, frac/4)*len(y_1)))\n",
    "        index = np.hstack([y_0,y_1])\n",
    "        np.random.shuffle(index)\n",
    "        x_new = x[index]\n",
    "        y_new = y[index]\n",
    "\n",
    "        X = np.vstack((X,x_new))\n",
    "        Y = np.hstack((Y,y_new))\n",
    "\n",
    "    X = X[1:,:]\n",
    "    Y = Y[1:]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dir DIR] [--input-dim INPUT_DIM]\n",
      "                             [--lr LR] [--alpha ALPHA] [--wd WD]\n",
      "                             [--batch-size BATCH_SIZE] [--epoch EPOCH]\n",
      "                             [--pre-epoch PRE_EPOCH] [--pretrain PRETRAIN]\n",
      "                             [--lamda LAMDA] [--beta BETA]\n",
      "                             [--hidden-dims HIDDEN_DIMS]\n",
      "                             [--latent-dim LATENT_DIM]\n",
      "                             [--n-clusters N_CLUSTERS]\n",
      "                             [--clustering CLUSTERING] [--n-jobs N_JOBS]\n",
      "                             [--device DEVICE] [--log-interval LOG_INTERVAL]\n",
      "                             [--test-run]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/shivin/Library/Jupyter/runtime/kernel-b2dc2ba9-68f0-4cc7-a5fa-fb77dfacba09.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "    parser = argparse.ArgumentParser(description='Deep Clustering Network')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--dir', default='../Dataset/sepsis/', \n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--input-dim', type=int, default=89, \n",
    "                        help='input dimension')\n",
    "\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--lr', type=float, default=0.002, \n",
    "                        help='learning rate (default: 1e-4)')\n",
    "    parser.add_argument('--alpha', type=float, default=0.04, \n",
    "                        help='alpha (default: 4e-2)')\n",
    "    parser.add_argument('--wd', type=float, default=5e-4, \n",
    "                        help='weight decay (default: 5e-4)')\n",
    "    parser.add_argument('--batch-size', type=int, default=256, \n",
    "                        help='input batch size for training')\n",
    "    parser.add_argument('--epoch', type=int, default=10, \n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--pre-epoch', type=int, default=5, \n",
    "                        help='number of pre-train epochs')\n",
    "    parser.add_argument('--pretrain', type=bool, default=True, \n",
    "                        help='whether use pre-training')\n",
    "    \n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--lamda', type=float, default=0.005,\n",
    "                        help='coefficient of the reconstruction loss')\n",
    "    parser.add_argument('--beta', type=float, default=1,\n",
    "                        help='coefficient of the regularization term on ' \\\n",
    "                            'clustering')\n",
    "    parser.add_argument('--hidden-dims', default=[500, 500, 2000],\n",
    "                        help='learning rate (default: 1e-4)')\n",
    "    parser.add_argument('--latent-dim', type=int, default=10,\n",
    "                        help='latent space dimension')\n",
    "    parser.add_argument('--n-clusters', type=int, default=2,\n",
    "                        help='number of clusters in the latent space')\n",
    "    parser.add_argument('--clustering', type=str, default='cac', \n",
    "                        help='choose a clustering method (default: kmeans)' \\\n",
    "                       ' meanshift, tba')\n",
    "\n",
    "\n",
    "    # Utility parameters\n",
    "    parser.add_argument('--n-jobs', type=int, default=6,\n",
    "                        help='number of jobs to run in parallel')\n",
    "    parser.add_argument('--device', type=str, default='cpu',\n",
    "                        help='device for computation (default: cpu)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100,\n",
    "                        help='how many batches to wait before logging the ' \\\n",
    "                            'training status')\n",
    "    parser.add_argument('--test-run', action='store_true',\n",
    "                        help='short test run on a few instances of the dataset')\n",
    "\n",
    "    args = parser.parse_args()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
